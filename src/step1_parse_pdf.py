"""
Step 1: ä½¿ç”¨ Docling è§£æ PDF ä¸¦è¼¸å‡º Markdown
"""
import sys
import traceback
from pathlib import Path
from typing import List, Dict
import json

# ç¢ºä¿èƒ½å°å…¥ config
sys.path.append(str(Path(__file__).parent.parent))
from config import (
    RAW_PDF_DIR,
    PROCESSED_MD_DIR,
    DOCLING_CONFIG,
    DOCLING_LAYERED_MODE,
    USE_GRANITE_DOCLING,
    DOCLING_DEVICE,
    DOCLING_NUM_THREADS,
    DOCLING_MAX_PAGES,
    DOCLING_IMAGES_SCALE,
)

try:
    from docling.document_converter import DocumentConverter, PdfFormatOption
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import ThreadedPdfPipelineOptions
except ImportError:
    print("âŒ è«‹å…ˆå®‰è£ docling: pip install docling")
    sys.exit(1)

if USE_GRANITE_DOCLING and not DOCLING_LAYERED_MODE:
    try:
        from docling.pipeline.vlm_pipeline import VlmPipeline
        from docling.datamodel.pipeline_options import VlmPipelineOptions
        from docling.datamodel.accelerator_options import AcceleratorOptions
    except ImportError as e:
        print("[WARN] Granite Docling éœ€ VLM æ”¯æ´ï¼Œæ”¹ç”¨é è¨­è§£æ:", e)
        USE_GRANITE_DOCLING = False

# åˆ†å±¤æ¨¡å¼ï¼šStep2 æœƒæŠŠ <!-- image --> æˆ– ![Image](...) æ›æˆ <image path="..." />
try:
    from docling_core.types.doc.base import ImageRefMode
except ImportError:
    ImageRefMode = None


class PDFParser:
    """ä½¿ç”¨ Docling è§£æ PDF çš„å°è£é¡åˆ¥ï¼ˆå¯é¸ Granite Docling VLMï¼‰"""
    
    def __init__(self):
        # å»ºç«‹ Docling è½‰æ›å™¨ï¼ˆåˆ†å±¤æ¨¡å¼ç”¨æ¨™æº– pipelineï¼Œä¸è·‘ VLMï¼‰
        if DOCLING_LAYERED_MODE:
            # å¿…é ˆé–‹å•Ÿ generate_picture_images æ‰æœƒæœ‰åœ–å¯åŒ¯å‡ºï¼›images_scale æ„ˆé«˜åœ–æ„ˆæ¸…æ™°
            pdf_opts = ThreadedPdfPipelineOptions(
                generate_picture_images=True,
                generate_page_images=True,
                images_scale=DOCLING_IMAGES_SCALE,
            )
            self.converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_opts),
                }
            )
            print("[OK] åˆ†å±¤æ¨¡å¼ï¼šæ¨™æº– Doclingï¼ˆæ¨™é¡Œ/æ®µè½ + åœ–åŒ¯å‡ºè‡³ images/ï¼‰")
        elif USE_GRANITE_DOCLING:
            # ä½¿ç”¨ Granite Doclingï¼ˆVLM pipelineï¼‰ï¼Œè£ç½®èˆ‡åŸ·è¡Œç·’ç”± config æ§åˆ¶
            accel = AcceleratorOptions(device=DOCLING_DEVICE, num_threads=DOCLING_NUM_THREADS)
            self.converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_cls=VlmPipeline,
                        pipeline_options=VlmPipelineOptions(accelerator_options=accel),
                    ),
                }
            )
            print(f"[OK] Granite Docling (VLM)ï¼Œè£ç½®: {DOCLING_DEVICE}ï¼ŒåŸ·è¡Œç·’: {DOCLING_NUM_THREADS}")
        else:
            self.converter = DocumentConverter()
        
    def parse_single_pdf(self, pdf_path: Path) -> Dict:
        """
        è§£æå–®ä¸€ PDF æª”æ¡ˆ
        
        Args:
            pdf_path: PDF æª”æ¡ˆè·¯å¾‘
            
        Returns:
            åŒ…å« markdown æ–‡æœ¬ã€metadata çš„å­—å…¸
        """
        print(f"ğŸ“„ é–‹å§‹è§£æ: {pdf_path.name}")
        if DOCLING_MAX_PAGES is not None:
            print(f"   ï¼ˆåƒ…å‰ {DOCLING_MAX_PAGES} é ï¼Œæ¸¬è©¦ç”¨ï¼‰")
        try:
            # åªè™•ç†å‰ N é ï¼šç”¨ page_rangeï¼Œä¸è¦ç”¨ max_num_pagesï¼ˆæœƒæŠŠå¤šé  PDF æ•´ä»½æ‹’æ”¶ï¼‰
            kwargs = {}
            if DOCLING_MAX_PAGES is not None:
                kwargs["page_range"] = (1, DOCLING_MAX_PAGES)
            result = self.converter.convert(str(pdf_path), **kwargs)
            
            if DOCLING_LAYERED_MODE and ImageRefMode is not None:
                # åˆ†å±¤ï¼šç›´æ¥å¯«å…¥ .md + åŒ¯å‡ºåœ–ç‰‡åˆ° images/{doc_stem}/ï¼ˆç›¸å°è·¯å¾‘ï¼Œæ–¹ä¾¿ Step2ï¼‰
                md_path = PROCESSED_MD_DIR / (pdf_path.stem + ".md")
                (PROCESSED_MD_DIR / "images" / pdf_path.stem).mkdir(parents=True, exist_ok=True)
                artifacts_dir = Path("images") / pdf_path.stem  # ç›¸å°è·¯å¾‘ï¼Œmd å…§ç‚º images/doc_stem/xxx.png
                result.document.save_as_markdown(
                    filename=md_path,
                    artifacts_dir=artifacts_dir,
                    image_mode=ImageRefMode.REFERENCED,
                )
                markdown_text = md_path.read_text(encoding="utf-8")
            else:
                markdown_text = result.document.export_to_markdown()
            
            # æå– metadata
            metadata = {
                "source": pdf_path.name,
                "num_pages": len(result.document.pages) if hasattr(result.document, 'pages') else 0,
                "title": getattr(result.document, 'title', pdf_path.stem),
            }
            
            return {
                "markdown": markdown_text,
                "metadata": metadata,
                "success": True
            }
            
        except Exception as e:
            print(f"âŒ è§£æå¤±æ•—: {e}")
            traceback.print_exc()
            return {
                "markdown": "",
                "metadata": {},
                "success": False,
                "error": str(e)
            }
    
    def save_markdown(self, pdf_path: Path, result: Dict):
        """å„²å­˜ Markdown åˆ°æª”æ¡ˆï¼ˆåˆ†å±¤æ¨¡å¼æ™‚ .md å·²åœ¨ parse æ™‚å¯«å…¥ï¼Œåªå¯« metadataï¼‰"""
        if not result["success"]:
            return
        
        output_path = PROCESSED_MD_DIR / (pdf_path.stem + ".md")
        if not DOCLING_LAYERED_MODE:
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(result["markdown"])
            print(f"âœ… å·²å„²å­˜: {output_path}")
        
        meta_path = PROCESSED_MD_DIR / (pdf_path.stem + "_meta.json")
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(result["metadata"], f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š Metadata: {meta_path}")
    
    def parse_directory(self, max_files: int = None) -> List[Dict]:
        """
        æ‰¹æ¬¡è§£æç›®éŒ„ä¸‹çš„æ‰€æœ‰ PDF
        
        Args:
            max_files: æœ€å¤šè™•ç†å¹¾å€‹æª”æ¡ˆï¼ˆæ¸¬è©¦ç”¨ï¼‰
            
        Returns:
            è§£æçµæœåˆ—è¡¨
        """
        pdf_files = list(RAW_PDF_DIR.glob("*.pdf"))
        
        if not pdf_files:
            print(f"âš ï¸  åœ¨ {RAW_PDF_DIR} æ‰¾ä¸åˆ° PDF æª”æ¡ˆ")
            return []
        
        if max_files:
            pdf_files = pdf_files[:max_files]
        
        print(f"ğŸ” æ‰¾åˆ° {len(pdf_files)} å€‹ PDF æª”æ¡ˆ")
        
        results = []
        for pdf_path in pdf_files:
            result = self.parse_single_pdf(pdf_path)
            if result["success"]:
                self.save_markdown(pdf_path, result)
            results.append(result)
        
        # çµ±è¨ˆ
        success_count = sum(1 for r in results if r["success"])
        print(f"\nğŸ“ˆ å®Œæˆ: {success_count}/{len(results)} å€‹æª”æ¡ˆæˆåŠŸè§£æ")
        
        return results


def main():
    """ä¸»ç¨‹å¼ - æ¸¬è©¦è§£æåŠŸèƒ½"""
    print("=" * 60)
    print("Step 1: PDF è§£ææ¸¬è©¦")
    print("=" * 60)
    
    parser = PDFParser()
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ PDF æª”æ¡ˆ
    pdf_files = list(RAW_PDF_DIR.glob("*.pdf"))
    if not pdf_files:
        print(f"\nâš ï¸  è«‹å…ˆå°‡ PDF æª”æ¡ˆæ”¾åˆ°: {RAW_PDF_DIR}")
        print("æç¤ºï¼šä½ å¯ä»¥å¾ LongDocURL è³‡æ–™é›†ä¸‹è¼‰æ¸¬è©¦æª”æ¡ˆ")
        return
    
    # å…ˆæ¸¬è©¦è§£æç¬¬ä¸€å€‹æª”æ¡ˆ
    print(f"\nğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šåªè§£æç¬¬ä¸€å€‹æª”æ¡ˆ")
    results = parser.parse_directory(max_files=1)
    
    if results and results[0]["success"]:
        print("\n" + "=" * 60)
        print("âœ… è§£ææˆåŠŸï¼å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„ Markdown æª”æ¡ˆ")
        print(f"è¼¸å‡ºç›®éŒ„: {PROCESSED_MD_DIR}")
        print("=" * 60)
    

if __name__ == "__main__":
    main()